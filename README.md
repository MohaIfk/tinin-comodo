# Le Miroir Virtuel

An interactive installation that captures and analyzes real-time gestures, facial expressions, and morphologies of people and animals via a webcam. Each detected entity is represented by a 3D avatar generated by AI and animated in real-time in TouchDesigner.

## Project Overview

Le Miroir Virtuel combines computer vision, artificial intelligence, and real-time visualization to create an immersive interactive experience. The system:

1. Detects and differentiates multiple people and animals in the camera's field of view
2. Generates stylized 3D avatars for each detected entity
3. Animates the avatars in TouchDesigner based on body movements and facial expressions
4. Displays the animated scene on an external screen or projector

## Requirements

### Hardware
- PC with NVIDIA GPU (recommended)
- HD Webcam
- External display or projector

### Software
- Python 3.10+
- TouchDesigner
- Required Python packages (see `requirements.txt`)

## Environment Setup and Installation

### Prerequisites
1. Ensure you have Python 3.10 or higher installed:
   ```
   python --version
   ```

2. Install TouchDesigner from [derivative.ca](https://derivative.ca/download)
   - Commercial or Non-Commercial license is required
   - Minimum version: 2022.28000 or newer

3. For optimal performance, ensure your NVIDIA GPU drivers are up-to-date
   - Visit [nvidia.com/drivers](https://www.nvidia.com/Download/index.aspx) to download the latest drivers

### Installation
1. Clone this repository:
   ```
   git clone https://github.com/yourusername/3dmesh.git
   cd 3dmesh
   ```

2. Create a virtual environment:
   ```
   python -m venv venv
   venv\Scripts\activate
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Download the YOLO model (if not already included):
   ```
   python -c "from ultralytics import YOLO; YOLO('yolov8n.pt')"
   ```

5. Set up your API key for avatar generation:
   ```
   set AVATAR_API_KEY=your_api_key_here
   ```

   For permanent setup, add it to your environment variables through Windows settings.

### Verifying Installation
To verify that all components are installed correctly:

1. Check Python and package versions:
   ```
   python -c "import cv2; print(f'OpenCV version: {cv2.__version__}')"
   python -c "import mediapipe; print(f'MediaPipe version: {mediapipe.__version__}')"
   python -c "import numpy; print(f'NumPy version: {numpy.__version__}')"
   python -c "import ultralytics; print(f'Ultralytics version: {ultralytics.__version__}')"
   ```

2. Test camera access:
   ```
   python -c "import cv2; cap = cv2.VideoCapture(0); print(f'Camera working: {cap.isOpened()}'); cap.release()"
   ```

3. Run the basic detection test:
   ```
   python test_fix.py
   ```

   You should see a window with camera feed and detection annotations.

4. Verify TouchDesigner integration:
   - Open TouchDesigner
   - Load the project file from td_project/main.toe
   - Run the integration test:
   ```
   python test_integration.py
   ```

5. Test avatar generation with reference images:
   ```
   python test_avatar_generator.py
   ```

   This test verifies reference image capture, API integration, and caching functionality.

6. Test TouchDesigner communication and avatar loading:
   ```
   python test_td_communication.py
   ```

   This test verifies OSC communication with TouchDesigner and sends test avatar data.

7. Test pose retargeting for human entities:
   ```
   python test_pose_retargeting.py
   ```

   This test demonstrates the pose retargeting functionality, showing how detected poses are mapped to avatar skeletons.

## Project Structure

- `core/` - Core functionality modules
  - `camera.py` - Camera input management
  - `detector.py` - Entity detection (humans and animals)
  - `avatar_generator.py` - AI-based avatar generation with reference image capture
  - `analyser.py` - Motion analysis and animation
  - `communicator.py` - TouchDesigner communication via OSC
- `config/` - Configuration files
  - `settings.py` - Application settings
  - `td_osc_mapping.json` - OSC address mapping for TouchDesigner
- `utils/` - Utility functions
- `td_project/` - TouchDesigner project files
- `assets/` - Resources and cached data
  - `avatars/` - Cached avatar data
  - `reference_images/` - Captured reference images of detected entities
  - `shaders/` - Shader files for TouchDesigner

## Usage

1. Start TouchDesigner and open the project file:
   ```
   td_project/main.toe
   ```

2. Run the main application:
   ```
   python main.py
   ```

3. For testing the integration of components:
   ```
   python test_integration.py
   ```

## Configuration

You can customize the application by modifying the settings in `config/settings.py`:

- Camera settings (index, resolution, FPS)
- Detection confidence thresholds
  - MP_DETECTION_CONFIDENCE: Confidence threshold for pose detection (default: 0.7)
  - MP_FACE_DETECTION_CONFIDENCE: Confidence threshold for face detection (default: 0.7)
  - MP_FACE_TRACKING_CONFIDENCE: Confidence threshold for face tracking (default: 0.5)
  - MP_MAX_NUM_FACES: Maximum number of faces to detect (default: 1)
- Entity validation parameters
  - MIN_ENTITY_SIZE: Minimum size as a fraction of frame dimensions (default: 0.05)
  - MAX_ENTITY_SIZE: Maximum size as a fraction of frame dimensions (default: 0.95)
  - VALID_POSITION_MARGIN: Margin from frame edges as a fraction of frame dimensions (default: 0.0)
  - Note: These parameters can be adjusted to fine-tune entity detection sensitivity
- Avatar API settings
- TouchDesigner connection settings
- Debug mode

## TouchDesigner Integration

The application communicates with TouchDesigner using OSC (Open Sound Control) messages. The OSC address mapping is defined in `config/td_osc_mapping.json`. The mapping includes:

- Avatar data (URL, type, species)
- Joint rotations for animation
- Facial expressions
- Body motion metrics

### OSC Communication

The system uses the `python-osc` library to send data to TouchDesigner. The communication is handled by the `TDCommunicator` class in `core/communicator.py`. The default configuration uses:

- IP Address: 127.0.0.1 (localhost)
- Port: 7000

You can modify these settings in `config/settings.py` if needed.

### Testing the TouchDesigner Link

To test the OSC communication with TouchDesigner:

1. Start TouchDesigner and open the project file:
   ```
   td_project/main.toe
   ```

2. Run the TouchDesigner communication test:
   ```
   python test_td_communication.py
   ```

3. Verify in TouchDesigner that the test avatars are received and displayed correctly.

### Avatar Loading in TouchDesigner

The TouchDesigner project is configured to load 3D avatar models from URLs sent via OSC. When an avatar URL is received, TouchDesigner will:

1. Download the 3D model (if not already cached)
2. Load the model into the scene
3. Apply animations based on the joint rotation and facial expression data

The system supports multiple avatars simultaneously, each with its own animation data.

## Features

### Entity Detection
- Human detection using MediaPipe (face and pose)
  - Comprehensive pose detection with all 33 body landmarks
  - Detailed face mesh detection with 468 facial landmarks
  - Enhanced emotion detection based on facial expressions
  - Configurable detection confidence thresholds
- Animal detection using YOLOv8
- Entity validation by size and position
  - Minimum and maximum size thresholds
  - Position validation to ensure entities are not too close to frame edges
  - Configurable validation parameters in settings.py

### Avatar Generation
- Automatic capture of face/morphology
- Reference image capture for improved avatar generation
- API integration with ReadyPlayerMe
- Support for different entity types (humans and animals)
- Intelligent caching of generated avatars with expiration
- Unique avatar generation based on entity features
- Automatic retrieval of 3D model files (.glb or .fbx)
  - Extraction of model URLs from API responses
  - Fallback to default models if API fails
  - Validation of model file extensions

### Real-time Animation
- Pose retargeting for accurate mapping of detected poses to avatar skeletons
- Joint rotation calculation for humans and animals
- Facial expression mapping from detected emotions
- Motion analysis for velocity and acceleration
- Smooth animation transitions

### TouchDesigner Visualization
- Dynamic loading of avatars
- Real-time animation via OSC
- Multi-entity support
- Visual effects and scene management

## GPU Acceleration

The project now includes comprehensive GPU acceleration support to improve performance when an NVIDIA GPU is available.

### GPU-Accelerated Components

- **YOLO Object Detection**: Animal detection using YOLOv8 is automatically accelerated when a compatible GPU is available
- **Frame Preprocessing**: Camera frames are preprocessed on the GPU for faster processing
- **PyTorch Operations**: All PyTorch-based operations use GPU acceleration when available

### Configuring GPU Usage

GPU acceleration can be configured in `config/settings.py`:

```python
# GPU/CUDA Settings
USE_GPU = True  # Set to False to force CPU usage even if GPU is available
```

The system automatically detects CUDA availability and configures components accordingly. You can disable GPU acceleration by setting `USE_GPU = False` in the settings file.

### Verifying GPU Usage

To verify that GPU acceleration is working correctly:

```
python test_gpu.py
```

This script will test all GPU-accelerated components and log detailed information about:
- CUDA availability
- GPU device name and memory
- OpenCV CUDA support
- Camera GPU acceleration
- YOLO model GPU acceleration

You can also see GPU information when running the main application:

```
python main.py
```

The application will log GPU information at startup, including:
- CUDA availability
- GPU device name and memory
- Whether components are using GPU acceleration

### GPU Requirements

- NVIDIA GPU with CUDA support
- Up-to-date NVIDIA drivers
- CUDA Toolkit 11.0 or newer (installed automatically with PyTorch)

## Troubleshooting

### Environment and Installation Issues
- **Python version issues**: Ensure you're using Python 3.10+. Earlier versions may not be compatible with all dependencies.
  ```
  python --version
  ```

- **Package installation failures**: 
  - Try updating pip: `python -m pip install --upgrade pip`
  - Install packages individually if batch installation fails
  - For Windows-specific issues with OpenCV, try: `pip install opencv-python-headless` instead

- **CUDA/GPU issues**:
  - Verify CUDA installation: `python -c "import torch; print(torch.cuda.is_available())"`
  - Check GPU drivers are up-to-date
  - If GPU acceleration isn't working, the system will fall back to CPU (slower performance)

- **Virtual environment problems**:
  - If you encounter permission issues: Run your terminal as Administrator
  - If venv activation fails: Ensure execution policy allows scripts `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

### Runtime Issues
- If the camera doesn't work, check the `CAMERA_INDEX` setting in `config/settings.py`
- If avatar generation fails, verify your API key is set correctly
- For TouchDesigner communication issues, check the IP and port settings
- If detection is slow or unstable, try lowering the camera resolution in settings.py

## License

This project is licensed under the MIT License - see the LICENSE file for details.
